{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gptSq2Xhcwq",
        "outputId": "95382559-7ac9-4757-9f6f-19fcfa7f29b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import itertools\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Importing libraries\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Bidirectional, Input, SpatialDropout1D, Concatenate, Add, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import SeparableConv1D, MultiHeadAttention, LayerNormalization, Attention\n",
        "from tensorflow.keras.layers import AveragePooling1D\n",
        "from tensorflow.keras.layers import Multiply\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "p4SVE3EXnADf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"### Load Data\"\"\"\n",
        "\n",
        "# Data directory\n",
        "DATADIR = '/content/drive/MyDrive/Colab Notebooks/UCI HAR Dataset'\n",
        "\n",
        "# Activity labels\n",
        "ACTIVITIES = {\n",
        "    1: 'WALKING',\n",
        "    2: 'WALKING_UPSTAIRS',\n",
        "    3: 'WALKING_DOWNSTAIRS',\n",
        "    4: 'SITTING',\n",
        "    5: 'STANDING',\n",
        "    6: 'LAYING'\n",
        "}\n",
        "\n",
        "# Raw data signals\n",
        "# Signals are from Accelerometer and Gyroscope\n",
        "# The signals are in x,y,z directions\n",
        "# Sensor signals are filtered to have only body acceleration\n",
        "# excluding the acceleration due to gravity\n",
        "# Triaxial acceleration from the accelerometer is total acceleration\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]"
      ],
      "metadata": {
        "id": "86x0Xu5ln2wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'{DATADIR}/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).to_numpy()\n",
        "        )\n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))"
      ],
      "metadata": {
        "id": "Cw5WYKqBn0hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_y(subset):\n",
        "    \"\"\"\n",
        "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
        "    that represents a human activity. We return a binary representation of\n",
        "    every sample objective as a 6 bits vector using One Hot Encoding\n",
        "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
        "    \"\"\"\n",
        "    filename = f'{DATADIR}/{subset}/y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).to_numpy(), y.values"
      ],
      "metadata": {
        "id": "peKubWa2nyrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Obtain the dataset from multiple files.\n",
        "    Returns: X_train, X_test, y_train, y_test, y_train_raw, y_test_raw\n",
        "    \"\"\"\n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    y_train, y_train_raw = load_y('train')\n",
        "    y_test, y_test_raw = load_y('test')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, y_train_raw, y_test_raw"
      ],
      "metadata": {
        "id": "f4KO-tYenxTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation functions\n",
        "def add_gaussian_noise(X, sigma=0.01):\n",
        "    \"\"\"Add random Gaussian noise to the data.\"\"\"\n",
        "    noise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
        "    return X + noise\n",
        "\n",
        "def apply_random_time_shift(X, shift_percent=0.05):\n",
        "    \"\"\"Apply random time shifts to the data (Â±5% of window length).\"\"\"\n",
        "    X_shifted = np.zeros_like(X)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        time_steps = X.shape[1]\n",
        "        features = X.shape[2]\n",
        "        max_shift = int(time_steps * shift_percent)\n",
        "\n",
        "        # Random shift between -max_shift and max_shift\n",
        "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
        "\n",
        "        # Apply shift\n",
        "        for j in range(features):\n",
        "            if shift > 0:\n",
        "                X_shifted[i, shift:, j] = X[i, :-shift, j]\n",
        "                # Pad the beginning\n",
        "                X_shifted[i, :shift, j] = X[i, 0, j]\n",
        "            elif shift < 0:\n",
        "                X_shifted[i, :shift, j] = X[i, -shift:, j]\n",
        "                # Pad the end\n",
        "                X_shifted[i, shift:, j] = X[i, -1, j]\n",
        "            else:\n",
        "                X_shifted[i, :, j] = X[i, :, j]\n",
        "\n",
        "    return X_shifted"
      ],
      "metadata": {
        "id": "jaIPbvtonv36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_training_data(X, y):\n",
        "    \"\"\"Apply multiple augmentation techniques to training data.\"\"\"\n",
        "    # Original data\n",
        "    X_augmented = [X]\n",
        "    y_augmented = [y]\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    X_noise = add_gaussian_noise(X, sigma=0.01)\n",
        "    X_augmented.append(X_noise)\n",
        "    y_augmented.append(y)\n",
        "\n",
        "    # Apply random time shifts\n",
        "    X_shifted = apply_random_time_shift(X, shift_percent=0.05)\n",
        "    X_augmented.append(X_shifted)\n",
        "    y_augmented.append(y)\n",
        "\n",
        "    # Concatenate all augmented data\n",
        "    X_final = np.concatenate(X_augmented, axis=0)\n",
        "    y_final = np.concatenate(y_augmented, axis=0)\n",
        "\n",
        "    print(f\"Original data shape: {X.shape}, Augmented data shape: {X_final.shape}\")\n",
        "\n",
        "    return X_final, y_final"
      ],
      "metadata": {
        "id": "0GorLO3fntbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights for imbalanced data\n",
        "def calculate_class_weights(y_raw):\n",
        "    \"\"\"Calculate inverse frequency weights to address class imbalance.\"\"\"\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_raw),\n",
        "        y=y_raw\n",
        "    )\n",
        "    return dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "sa-QoRrTnrhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced CSV data processing for external data\n",
        "def process_csv_data(file_path, train_mean=None, train_std=None):\n",
        "    \"\"\"\n",
        "    Process external CSV data for inference\n",
        "    Returns normalized data in the correct shape for the model\n",
        "    \"\"\"\n",
        "    # Load CSV data with actual columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Check for required columns\n",
        "    required_columns = ['gFx', 'gFy', 'gFz', 'wx', 'wy', 'wz', 'ax', 'ay', 'az']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    # Map CSV columns to required features\n",
        "    raw_data = np.hstack([\n",
        "        df[['gFx', 'gFy', 'gFz']].values,  # Body acceleration\n",
        "        df[['wx', 'wy', 'wz']].values,     # Gyroscope\n",
        "        df[['ax', 'ay', 'az']].values      # Total acceleration\n",
        "    ])\n",
        "\n",
        "    # Normalize if mean and std are provided\n",
        "    if train_mean is not None and train_std is not None:\n",
        "        raw_data = (raw_data - train_mean) / train_std\n",
        "\n",
        "    # Create windows of 128 timesteps\n",
        "    windows = []\n",
        "    for i in range(0, len(raw_data) - 127, 32):  # Stride of 32 for less overlap\n",
        "        window = raw_data[i:i+128]\n",
        "        if len(window) == 128:  # Ensure complete window\n",
        "            windows.append(window)\n",
        "\n",
        "    if not windows:\n",
        "        raise ValueError(\"Not enough data to create windows\")\n",
        "\n",
        "    return np.array(windows)"
      ],
      "metadata": {
        "id": "8zCFisJBnpyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal smoothing for predictions\n",
        "def apply_temporal_smoothing(predictions, window_size=5):\n",
        "    \"\"\"\n",
        "    Apply temporal smoothing to model predictions to reduce noise.\n",
        "    Uses a sliding window average approach.\n",
        "    \"\"\"\n",
        "    smoothed_predictions = np.zeros_like(predictions)\n",
        "\n",
        "    # For each prediction, take the average of surrounding predictions\n",
        "    for i in range(len(predictions)):\n",
        "        # Define window bounds\n",
        "        start = max(0, i - window_size // 2)\n",
        "        end = min(len(predictions), i + window_size // 2 + 1)\n",
        "\n",
        "        # Calculate average within window\n",
        "        window_preds = predictions[start:end]\n",
        "        smoothed_predictions[i] = np.mean(window_preds, axis=0)\n",
        "\n",
        "    return smoothed_predictions"
      ],
      "metadata": {
        "id": "vVlwRm6qnndj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing for predictions\n",
        "def post_process_predictions(predictions, threshold=0.5, smoothing_window=5):\n",
        "    \"\"\"\n",
        "    Apply post-processing to raw predictions:\n",
        "    1. Threshold probabilities\n",
        "    2. Apply smoothing to reduce noise\n",
        "    3. Return predicted class indices and labels\n",
        "    \"\"\"\n",
        "    # Apply temporal smoothing\n",
        "    smoothed_preds = apply_temporal_smoothing(predictions, smoothing_window)\n",
        "\n",
        "    # Get predicted class indices\n",
        "    pred_indices = np.argmax(smoothed_preds, axis=1)\n",
        "\n",
        "    # Apply smoothing with a sliding window\n",
        "    if len(pred_indices) >= smoothing_window:\n",
        "        smoothed_indices = np.zeros_like(pred_indices)\n",
        "        for i in range(len(pred_indices)):\n",
        "            start = max(0, i - smoothing_window // 2)\n",
        "            end = min(len(pred_indices), i + smoothing_window // 2 + 1)\n",
        "            window = pred_indices[start:end]\n",
        "            # Most common class in window\n",
        "            values, counts = np.unique(window, return_counts=True)\n",
        "            smoothed_indices[i] = values[np.argmax(counts)]\n",
        "        pred_indices = smoothed_indices\n",
        "\n",
        "    # Map indices to activity labels (add 1 because activities are 1-indexed)\n",
        "    pred_labels = [ACTIVITIES[idx + 1] for idx in pred_indices]\n",
        "\n",
        "    return pred_indices, pred_labels"
      ],
      "metadata": {
        "id": "A-c5ayx_nl6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layers for attention mechanisms\n",
        "class TemporalAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"Custom attention mechanism for time series data.\"\"\"\n",
        "    def __init__(self, units=64, **kwargs):\n",
        "        super(TemporalAttention, self).__init__(**kwargs)\n",
        "        self.W1 = Dense(units, activation='tanh')\n",
        "        self.W2 = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Reshape to 2D for dense layer application\n",
        "        x = tf.reshape(inputs, (-1, inputs.shape[2]))\n",
        "\n",
        "        # Apply attention weights calculation\n",
        "        x = self.W1(x)\n",
        "        x = self.W2(x)\n",
        "\n",
        "        # Reshape attention weights back to sequence form\n",
        "        attention_weights = tf.reshape(x, (-1, inputs.shape[1], 1))\n",
        "        attention_weights = tf.nn.softmax(attention_weights, axis=1)\n",
        "\n",
        "        # Apply attention weights to the input\n",
        "        context_vector = inputs * attention_weights\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config"
      ],
      "metadata": {
        "id": "nweMOxaRniNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization functions\n",
        "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', cmap=None, figsize=(10, 8), normalize=False):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title += ' (Normalized)'\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.cm.Blues\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45, fontsize=12)\n",
        "    plt.yticks(tick_marks, class_names, fontsize=12)\n",
        "\n",
        "    # Add text annotations to confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label', fontsize=14)\n",
        "    plt.xlabel('Predicted label', fontsize=14)\n",
        "    return plt"
      ],
      "metadata": {
        "id": "pSapUSx2ngff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_classification_metrics(y_true, y_pred, class_names, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Plot precision, recall, and F1 scores from classification results\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    # Prepare data\n",
        "    metrics = [precision, recall, f1]\n",
        "    titles = ['Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "    # Create bar charts\n",
        "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        axes[i].bar(np.arange(len(class_names)), metric, color='skyblue')\n",
        "        axes[i].set_title(title, fontsize=16)\n",
        "        axes[i].set_xticks(np.arange(len(class_names)))\n",
        "        axes[i].set_xticklabels(class_names, rotation=45, ha='right', fontsize=12)\n",
        "        axes[i].set_ylim(0, 1.0)\n",
        "        axes[i].set_xlabel('Activity', fontsize=14)\n",
        "        axes[i].set_ylabel('Score', fontsize=14)\n",
        "\n",
        "        # Add value labels\n",
        "        for j, v in enumerate(metric):\n",
        "            axes[i].text(j, v + 0.02, f'{v:.2f}', ha='center', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "KK1_Na0anex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_combined_metrics(y_true, y_pred, class_names, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Plot a combined view of precision, recall, and F1 scores\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Set up positions for grouped bars\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.25\n",
        "\n",
        "    # Create bar groups\n",
        "    plt.bar(x - width, precision, width, label='Precision', color='#5DA5DA')\n",
        "    plt.bar(x, recall, width, label='Recall', color='#FAA43A')\n",
        "    plt.bar(x + width, f1, width, label='F1 Score', color='#60BD68')\n",
        "\n",
        "    # Add labels and titles\n",
        "    plt.title('Classification Performance Metrics by Activity', fontsize=16)\n",
        "    plt.xlabel('Activity', fontsize=14)\n",
        "    plt.ylabel('Score', fontsize=14)\n",
        "    plt.xticks(x, class_names, rotation=45, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    return plt"
      ],
      "metadata": {
        "id": "ZhJrNbkWnciU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure session settings\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "\n",
        "# Start a session\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)"
      ],
      "metadata": {
        "id": "FaqLPahKoEv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initializing parameters\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "n_hidden = 32"
      ],
      "metadata": {
        "id": "Jo50wvbKnawZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))\n",
        "\n",
        "# Loading the train and test data\n",
        "X_train, X_test, Y_train, Y_test, Y_train_raw, Y_test_raw = load_data()\n",
        "\n",
        "timesteps = len(X_train[0])\n",
        "input_dim = len(X_train[0][0])\n",
        "n_classes = _count_classes(Y_train)\n",
        "\n",
        "print(f\"Timesteps: {timesteps}\")\n",
        "print(f\"Input dimensions: {input_dim}\")\n",
        "print(f\"Number of training samples: {len(X_train)}\")\n",
        "print(f\"Shape of training data: {X_train.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ToTiduVEnXFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment training data - Apply Gaussian noise and random time shifts\n",
        "X_train_aug, Y_train_aug = augment_training_data(X_train, Y_train)\n",
        "\n",
        "# Calculate class weights to address imbalance\n",
        "class_weights = calculate_class_weights(Y_train_raw)\n",
        "print(\"Class weights to address imbalance:\", class_weights)\n",
        "\n",
        "# Calculate and store normalization parameters for external data processing\n",
        "train_mean = np.mean(X_train.reshape(-1, input_dim), axis=0)\n",
        "train_std = np.std(X_train.reshape(-1, input_dim), axis=0)\n",
        "np.save('train_normalization_params.npy', {'mean': train_mean, 'std': train_std})\n"
      ],
      "metadata": {
        "id": "V3COxCrbnU0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## Defining LSTM Network model:\"\"\"\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_lstm_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Add ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "-QtLyCKJnRPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create LSTM model (for ensemble)\n",
        "def create_lstm_with_attention():\n",
        "    inputs = Input(shape=(timesteps, input_dim))\n",
        "\n",
        "    # First LSTM layer with increased units (128) and return_sequences=True\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = SpatialDropout1D(0.5)(x)  # Spatial Dropout instead of regular Dropout\n",
        "\n",
        "    # Second LSTM layer\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = SpatialDropout1D(0.5)(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attn = TemporalAttention(64)(x)\n",
        "\n",
        "    # Dense layer with ReLU activation and L2 regularization\n",
        "    x = Dense(n_hidden, activation='relu', kernel_regularizer=l2(0.01))(attn)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    outputs = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Print the summary of the LSTM model (model1)\n",
        "print(\"\\n=== LSTM Model (model1) Summary ===\")\n",
        "model1 = create_lstm_with_attention()\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "24mCScKBnPAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an ensemble of LSTM models\n",
        "def create_lstm_ensemble(num_models=3):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        model = create_lstm_with_attention()\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "# Ensemble prediction function\n",
        "def ensemble_predict(models, X):\n",
        "    \"\"\"Combine predictions from multiple models using averaging.\"\"\"\n",
        "    predictions = [model.predict(X) for model in models]\n",
        "    # Average the predictions\n",
        "    ensemble_pred = np.mean(predictions, axis=0)\n",
        "    return ensemble_pred\n",
        "# Create ensemble of LSTM models\n",
        "lstm_ensemble = create_lstm_ensemble(3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VetGP5EanMuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train each model in the ensemble\n",
        "lstm_histories = []\n",
        "for i, model in enumerate(lstm_ensemble):\n",
        "    print(f\"\\nTraining LSTM Ensemble Model {i+1}/{len(lstm_ensemble)}\")\n",
        "\n",
        "    # Train with early stopping, checkpoints, and reduce LR\n",
        "    history = model.fit(\n",
        "        X_train_aug,\n",
        "        Y_train_aug,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_test, Y_test),\n",
        "        epochs=epochs,\n",
        "        class_weight=class_weights,  # Use class weights to address imbalance\n",
        "        callbacks=[early_stopping,\n",
        "                   ModelCheckpoint(f'best_lstm_ensemble_{i}.keras', monitor='val_accuracy', save_best_only=True),\n",
        "                   reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    lstm_histories.append(history)\n",
        "\n",
        "    # Save model\n",
        "    model.save(f'lstm_ensemble_model_{i}.keras')\n",
        "\n"
      ],
      "metadata": {
        "id": "bQc5eDEwnLUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ensemble\n",
        "ensemble_predictions = ensemble_predict(lstm_ensemble, X_test)\n",
        "y_pred_classes = np.argmax(ensemble_predictions, axis=1)\n",
        "y_true_classes = np.argmax(Y_test, axis=1)\n",
        "# Evaluate final ensemble performance\n",
        "ensemble_accuracy = np.mean([np.equal(y_pred_classes, y_true_classes).astype(float)])\n",
        "print(f\"\\nLSTM Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "IKs_Dw-LnIza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get activity names for visualization\n",
        "activity_names = [ACTIVITIES[i+1] for i in range(n_classes)]\n",
        "\n",
        "# Generate and display enhanced confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "# Regular confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='LSTM Ensemble Confusion Matrix', figsize=(12, 10))\n",
        "plt.savefig('lstm_ensemble_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8IBtx1PnHZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='LSTM Ensemble Confusion Matrix', figsize=(12, 10), normalize=True)\n",
        "plt.savefig('lstm_ensemble_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qVRgCU8anFm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display text classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=activity_names))\n"
      ],
      "metadata": {
        "id": "-apxpU7EnD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot graphical classification report\n",
        "plot_classification_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('lstm_ensemble_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VJmJ6AgenC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot combined metrics\n",
        "plot_combined_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('lstm_ensemble_combined_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GHju4SDCnByv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## Defining CNN-LSTM Network model:\"\"\"\n",
        "\n",
        "# Reshape data for CNN-LSTM model\n",
        "n_steps, n_length = 4, 32\n",
        "X_train_cnn = X_train_aug.reshape((X_train_aug.shape[0], n_steps, n_length, input_dim))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], n_steps, n_length, input_dim))\n",
        "\n",
        "print(f\"Reshaped data for CNN-LSTM: {X_train_cnn.shape}\")"
      ],
      "metadata": {
        "id": "9EIgYxXamJGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_lstm_hierarchical():\n",
        "    inputs = Input(shape=(n_steps, n_length, input_dim))\n",
        "\n",
        "    # First level: Efficient convolution with more focused filters\n",
        "    x = TimeDistributed(SeparableConv1D(filters=64, kernel_size=5, activation='relu', padding='same'))(inputs)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(SpatialDropout1D(0.25))(x)  # Reduced dropout for more training signal\n",
        "\n",
        "    # Residual connection - first branch\n",
        "    skip_connection = x\n",
        "\n",
        "    # Second level: Add dilated convolution for better temporal pattern recognition without more parameters\n",
        "    x = TimeDistributed(SeparableConv1D(filters=64, kernel_size=3, dilation_rate=2,\n",
        "                                       activation='relu', padding='same'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "\n",
        "    # Apply residual connection for better gradient flow\n",
        "    if skip_connection.shape[-1] == x.shape[-1]:\n",
        "        x = Add()([x, skip_connection])\n",
        "    else:\n",
        "        # Project to match dimensions if needed\n",
        "        skip_proj = TimeDistributed(Conv1D(filters=64, kernel_size=1, padding='same'))(skip_connection)\n",
        "        x = Add()([x, skip_proj])\n",
        "\n",
        "    # Efficient feature extraction: Two complementary pooling methods\n",
        "    # Small scale features (local patterns)\n",
        "    small_pool = TimeDistributed(MaxPooling1D(pool_size=2))(x)\n",
        "    small_pool = TimeDistributed(SpatialDropout1D(0.25))(small_pool)\n",
        "\n",
        "    # Larger scale features (global patterns)\n",
        "    large_pool = TimeDistributed(GlobalAveragePooling1D())(x)\n",
        "    large_pool_reshaped = TimeDistributed(Reshape((1, 64)))(large_pool)\n",
        "\n",
        "    # Flatten small scale features\n",
        "    small_flat = TimeDistributed(Flatten())(small_pool)\n",
        "\n",
        "    # Feature combination\n",
        "    combined_features = small_flat\n",
        "\n",
        "    # Efficient attention mechanism\n",
        "    attention_layer = TemporalAttention(units=32)(combined_features)\n",
        "    attention_weighted = Multiply()([combined_features, attention_layer])\n",
        "\n",
        "    # Efficient LSTM implementation with careful tuning\n",
        "    # Single bidirectional LSTM with focused units - removed recurrent_regularizer which may not be supported\n",
        "    lstm_out = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.1))(attention_weighted)\n",
        "    lstm_out = LayerNormalization()(lstm_out)\n",
        "\n",
        "    # Add efficient self-attention for temporal dependencies\n",
        "    lstm_attention = MultiHeadAttention(num_heads=2, key_dim=32)(lstm_out, lstm_out)\n",
        "    lstm_out = Add()([lstm_out, lstm_attention])\n",
        "    lstm_out = LayerNormalization()(lstm_out)\n",
        "\n",
        "    # Global pooling to reduce sequence dimension\n",
        "    lstm_out = GlobalAveragePooling1D()(lstm_out)\n",
        "    lstm_out = Dropout(0.25)(lstm_out)\n",
        "\n",
        "    # Efficient representation layer\n",
        "    x = Dense(96, activation='relu', kernel_regularizer=l2(0.001))(lstm_out)  # Changed swish to relu for compatibility\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Print the summary of the CNN-LSTM model (model2)\n",
        "print(\"\\n=== CNN-LSTM Model (model2) Summary ===\")\n",
        "model2 = create_cnn_lstm_hierarchical()\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "5V8lUqw1mzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an ensemble of CNN-LSTM models\n",
        "def create_cnn_lstm_ensemble(num_models=3):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        model = create_cnn_lstm_hierarchical()\n",
        "        models.append(model)\n",
        "    return models\n",
        "# Create ensemble of CNN-LSTM models\n",
        "cnn_lstm_ensemble = create_cnn_lstm_ensemble(3)\n",
        "\n",
        "# Modify early stopping to be more patient for accuracy but with limits for free GPU\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,  # Balanced patience for accuracy vs compute time\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Improved learning rate scheduler for better convergence on limited compute\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.6,\n",
        "    patience=3,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "d8pwDtjemsfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More efficient augmentation that targets accuracy improvements\n",
        "def advanced_augment_training_data(X, y):\n",
        "    \"\"\"Apply targeted data augmentation for CNN-LSTM model\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    y_aug = y.copy()\n",
        "\n",
        "    # Gaussian noise with strategic intensity\n",
        "    X_noise = add_gaussian_noise(X, sigma=0.015)\n",
        "    X_aug = np.vstack((X_aug, X_noise))\n",
        "    y_aug = np.vstack((y_aug, y))\n",
        "\n",
        "    # Strategic time shifts focusing on most relevant movement variations\n",
        "    X_shift = apply_random_time_shift(X, shift_percent=0.06)\n",
        "    X_aug = np.vstack((X_aug, X_shift))\n",
        "    y_aug = np.vstack((y_aug, y))\n",
        "\n",
        "    # Apply more focused augmentation for underrepresented classes\n",
        "    class_counts = np.sum(y, axis=0)\n",
        "    max_count = np.max(class_counts)\n",
        "\n",
        "    for class_idx in range(len(class_counts)):\n",
        "        if class_counts[class_idx] < max_count * 0.6:  # Target more classes for balanced learning\n",
        "            # Find samples of this class\n",
        "            class_indices = np.where(y[:, class_idx] == 1)[0]\n",
        "            if len(class_indices) > 0:\n",
        "                # Select a subset of samples for efficiency\n",
        "                selection_size = min(len(class_indices), 20)  # Limit number of samples\n",
        "                selected_indices = np.random.choice(class_indices, selection_size, replace=False)\n",
        "                class_samples = X[selected_indices]\n",
        "                class_labels = y[selected_indices]\n",
        "\n",
        "                # Strategic noise addition for better generalization\n",
        "                augmented = add_gaussian_noise(class_samples, sigma=0.02)\n",
        "                X_aug = np.vstack((X_aug, augmented))\n",
        "                y_aug = np.vstack((y_aug, class_labels))\n",
        "\n",
        "    return X_aug, y_aug"
      ],
      "metadata": {
        "id": "bLWbHmpVmn28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train each model in the ensemble with enhanced augmentation\n",
        "cnn_lstm_histories = []\n",
        "for i, model in enumerate(cnn_lstm_ensemble):\n",
        "    print(f\"\\nTraining Enhanced CNN-LSTM Ensemble Model {i+1}/{len(cnn_lstm_ensemble)}\")\n",
        "\n",
        "    # Apply advanced augmentation\n",
        "    X_train_cnn_aug, Y_train_cnn_aug = advanced_augment_training_data(X_train_cnn, Y_train_aug)\n",
        "\n",
        "    # Define checkpoint for this model with efficiency settings for Colab\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        f'best_cnn_lstm_ensemble_{i}.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train with early stopping, checkpoints, and reduce LR\n",
        "    history = model.fit(\n",
        "        X_train_cnn_aug,\n",
        "        Y_train_cnn_aug,\n",
        "        batch_size=48,  # Balanced batch size for T4 GPU memory\n",
        "        validation_data=(X_test_cnn, Y_test),\n",
        "        epochs=epochs,  # Fewer epochs for Colab free GPU limit\n",
        "        class_weight=class_weights,  # Use class weights to address imbalance\n",
        "        callbacks=[early_stopping, checkpoint, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    cnn_lstm_histories.append(history)"
      ],
      "metadata": {
        "id": "TG558oxfmlxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ensemble\n",
        "ensemble_predictions = ensemble_predict(cnn_lstm_ensemble, X_test_cnn)\n",
        "\n",
        "# Apply temporal smoothing to ensemble predictions\n",
        "smoothed_predictions = apply_temporal_smoothing(ensemble_predictions)\n",
        "y_pred_classes = np.argmax(smoothed_predictions, axis=1)"
      ],
      "metadata": {
        "id": "2gsg8MTemj5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate final ensemble performance\n",
        "ensemble_accuracy = np.mean([np.equal(y_pred_classes, y_true_classes).astype(float)])\n",
        "print(f\"\\nCNN-LSTM Ensemble Accuracy: {ensemble_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4PugsRbhmiSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and display enhanced confusion matrix for CNN-LSTM\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "# Regular confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='CNN-LSTM Ensemble Confusion Matrix', figsize=(12, 10))\n",
        "plt.savefig('cnn_lstm_ensemble_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QIxA7Q1Wmda-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='CNN-LSTM Ensemble Confusion Matrix', figsize=(12, 10), normalize=True)\n",
        "plt.savefig('cnn_lstm_ensemble_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-cAxyW0ymcYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display text classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=activity_names))"
      ],
      "metadata": {
        "id": "W-DAtqOVmbNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot graphical classification report\n",
        "plot_classification_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('cnn_lstm_ensemble_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9M4lsu_3mZV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot combined metrics\n",
        "plot_combined_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('cnn_lstm_ensemble_combined_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EEoJ5Ac6mYbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_history(histories, model_name):\n",
        "    \"\"\"\n",
        "    Plots the training and validation accuracy and loss curves for ensemble models.\n",
        "\n",
        "    Args:\n",
        "        histories: List of training history objects returned by model.fit.\n",
        "        model_name: The name of the model type (e.g., \"LSTM\", \"CNN-LSTM\").\n",
        "    \"\"\"\n",
        "    # Check if histories list is empty\n",
        "    if not histories:\n",
        "        print(f\"Warning: No training histories found for {model_name}\")\n",
        "        return\n",
        "\n",
        "    # Ensure all histories have the expected keys\n",
        "    required_keys = ['accuracy', 'val_accuracy', 'loss', 'val_loss']\n",
        "    for i, h in enumerate(histories):\n",
        "        if not all(key in h.history for key in required_keys):\n",
        "            print(f\"Warning: Model {i+1} history is missing required keys. Skipping plot for {model_name}.\")\n",
        "            return\n",
        "\n",
        "    # Check if all histories have consistent lengths\n",
        "    first_history_len = len(histories[0].history['accuracy'])\n",
        "    if not all(len(h.history['accuracy']) == first_history_len for h in histories):\n",
        "        print(f\"Warning: Inconsistent history lengths for {model_name}. Adjusting to shortest length.\")\n",
        "        min_len = min(len(h.history['accuracy']) for h in histories)\n",
        "\n",
        "        # Calculate average history using the minimum length\n",
        "        avg_history = {\n",
        "            'accuracy': np.mean([[h.history['accuracy'][i] for h in histories] for i in range(min_len)], axis=1),\n",
        "            'val_accuracy': np.mean([[h.history['val_accuracy'][i] for h in histories] for i in range(min_len)], axis=1),\n",
        "            'loss': np.mean([[h.history['loss'][i] for h in histories] for i in range(min_len)], axis=1),\n",
        "            'val_loss': np.mean([[h.history['val_loss'][i] for h in histories] for i in range(min_len)], axis=1),\n",
        "        }\n",
        "    else:\n",
        "        # Calculate average history as before\n",
        "        avg_history = {\n",
        "            'accuracy': np.mean([[h.history['accuracy'][i] for h in histories] for i in range(first_history_len)], axis=1),\n",
        "            'val_accuracy': np.mean([[h.history['val_accuracy'][i] for h in histories] for i in range(first_history_len)], axis=1),\n",
        "            'loss': np.mean([[h.history['loss'][i] for h in histories] for i in range(first_history_len)], axis=1),\n",
        "            'val_loss': np.mean([[h.history['val_loss'][i] for h in histories] for i in range(first_history_len)], axis=1),\n",
        "        }\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    axs[0].plot(avg_history['accuracy'], linewidth=2, label='Avg Train')\n",
        "    axs[0].plot(avg_history['val_accuracy'], linewidth=2, label='Avg Validation')\n",
        "\n",
        "    # Plot individual model histories\n",
        "    for i, hist in enumerate(histories):\n",
        "        # Ensure we only plot up to the calculated length\n",
        "        hist_len = len(hist.history['accuracy'])\n",
        "        actual_len = min(hist_len, len(avg_history['accuracy']))\n",
        "\n",
        "        axs[0].plot(hist.history['accuracy'][:actual_len], linewidth=1, alpha=0.3, linestyle='--', label=f'Model {i+1} Train')\n",
        "        axs[0].plot(hist.history['val_accuracy'][:actual_len], linewidth=1, alpha=0.3, linestyle='--', label=f'Model {i+1} Val')\n",
        "\n",
        "    axs[0].set_title(f'{model_name} Ensemble Model Accuracy', fontsize=16)\n",
        "    axs[0].set_ylabel('Accuracy', fontsize=14)\n",
        "    axs[0].set_xlabel('Epoch', fontsize=14)\n",
        "    axs[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "    axs[0].legend(fontsize=10)\n",
        "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    axs[1].plot(avg_history['loss'], linewidth=2, label='Avg Train')\n",
        "    axs[1].plot(avg_history['val_loss'], linewidth=2, label='Avg Validation')\n",
        "\n",
        "    # Plot individual model histories\n",
        "    for i, hist in enumerate(histories):\n",
        "        # Ensure we only plot up to the calculated length\n",
        "        hist_len = len(hist.history['loss'])\n",
        "        actual_len = min(hist_len, len(avg_history['loss']))\n",
        "\n",
        "        axs[1].plot(hist.history['loss'][:actual_len], linewidth=1, alpha=0.3, linestyle='--', label=f'Model {i+1} Train')\n",
        "        axs[1].plot(hist.history['val_loss'][:actual_len], linewidth=1, alpha=0.3, linestyle='--', label=f'Model {i+1} Val')\n",
        "\n",
        "    axs[1].set_title(f'{model_name} Ensemble Model Loss', fontsize=16)\n",
        "    axs[1].set_ylabel('Loss', fontsize=14)\n",
        "    axs[1].set_xlabel('Epoch', fontsize=14)\n",
        "    axs[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "    axs[1].legend(fontsize=10)\n",
        "    axs[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(f'{model_name.lower().replace(\" \", \"_\")}_ensemble_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bui-oVwsmVY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training history graphs\n",
        "plot_model_history(lstm_histories, \"Improved LSTM\")\n",
        "plot_model_history(cnn_lstm_histories, \"Improved CNN-LSTM\")\n",
        "\n",
        "# Model performance comparison\n",
        "lstm_acc = ensemble_accuracy\n",
        "cnn_lstm_acc = np.mean([np.equal(y_pred_classes, y_true_classes).astype(float)])\n",
        "\n",
        "# Compare model performance with a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "models = ['LSTM Ensemble', 'CNN-LSTM Ensemble']\n",
        "accuracies = [lstm_acc, cnn_lstm_acc]\n",
        "# Create a gradient-colored bar chart\n",
        "plt.bar(models, accuracies, color=['#3498db', '#2ecc71'], alpha=0.8, width=0.6)\n",
        "plt.title('Ensemble Model Accuracy Comparison', fontsize=16)\n",
        "plt.ylabel('Test Accuracy', fontsize=14)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)"
      ],
      "metadata": {
        "id": "CJ8MVKZ7mPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add value labels\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_model_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n_zLrix2mLG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}