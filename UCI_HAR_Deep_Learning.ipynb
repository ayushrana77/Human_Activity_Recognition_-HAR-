{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "WRvC9OMX9ugz"
      },
      "source": [
        "# UCI - Human Action Recognition - Deep-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyF0DKjD90Wf",
        "outputId": "be7fd783-94d6-49d5-9f1e-3e9475f81b46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yfdiHSS2tnq",
        "outputId": "ad9f1775-5a0a-48e2-dd79-68e63d2dbf3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UeivSeuXAUaV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ta6qxJNZAXSx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6FD1gV1rAacT"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Bidirectional\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vbM2s69IAdqh"
      },
      "outputs": [],
      "source": [
        "\"\"\"### Load Data\"\"\"\n",
        "\n",
        "# Data directory\n",
        "DATADIR = '/content/drive/MyDrive/Colab Notebooks/UCI HAR Dataset'\n",
        "\n",
        "# Activity labels\n",
        "ACTIVITIES = {\n",
        "    1: 'WALKING',\n",
        "    2: 'WALKING_UPSTAIRS',\n",
        "    3: 'WALKING_DOWNSTAIRS',\n",
        "    4: 'SITTING',\n",
        "    5: 'STANDING',\n",
        "    6: 'LAYING'\n",
        "}\n",
        "\n",
        "# Raw data signals\n",
        "# Signals are from Accelerometer and Gyroscope\n",
        "# The signals are in x,y,z directions\n",
        "# Sensor signals are filtered to have only body acceleration\n",
        "# excluding the acceleration due to gravity\n",
        "# Triaxial acceleration from the accelerometer is total acceleration\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rIvI1UQzAjY4"
      },
      "outputs": [],
      "source": [
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'{DATADIR}/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).to_numpy()\n",
        "        )\n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aZ8MWTLnAmeS"
      },
      "outputs": [],
      "source": [
        "def load_y(subset):\n",
        "    \"\"\"\n",
        "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
        "    that represents a human activity. We return a binary representation of\n",
        "    every sample objective as a 6 bits vector using One Hot Encoding\n",
        "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
        "    \"\"\"\n",
        "    filename = f'{DATADIR}/{subset}/y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).to_numpy(), y.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VVu_IHlhApUg"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Obtain the dataset from multiple files.\n",
        "    Returns: X_train, X_test, y_train, y_test, y_train_raw, y_test_raw\n",
        "    \"\"\"\n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    y_train, y_train_raw = load_y('train')\n",
        "    y_test, y_test_raw = load_y('test')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, y_train_raw, y_test_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rsufsI3IAr0F"
      },
      "outputs": [],
      "source": [
        "# Enhanced CSV data processing for external data\n",
        "def process_csv_data(file_path, train_mean=None, train_std=None):\n",
        "    \"\"\"\n",
        "    Process external CSV data for inference\n",
        "    Returns normalized data in the correct shape for the model\n",
        "    \"\"\"\n",
        "    # Load CSV data with actual columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Check for required columns\n",
        "    required_columns = ['gFx', 'gFy', 'gFz', 'wx', 'wy', 'wz', 'ax', 'ay', 'az']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    # Map CSV columns to required features\n",
        "    raw_data = np.hstack([\n",
        "        df[['gFx', 'gFy', 'gFz']].values,  # Body acceleration\n",
        "        df[['wx', 'wy', 'wz']].values,     # Gyroscope\n",
        "        df[['ax', 'ay', 'az']].values      # Total acceleration\n",
        "    ])\n",
        "\n",
        "    # Normalize if mean and std are provided\n",
        "    if train_mean is not None and train_std is not None:\n",
        "        raw_data = (raw_data - train_mean) / train_std\n",
        "\n",
        "    # Create windows of 128 timesteps\n",
        "    windows = []\n",
        "    for i in range(0, len(raw_data) - 127, 32):  # Stride of 32 for less overlap\n",
        "        window = raw_data[i:i+128]\n",
        "        if len(window) == 128:  # Ensure complete window\n",
        "            windows.append(window)\n",
        "\n",
        "    if not windows:\n",
        "        raise ValueError(\"Not enough data to create windows\")\n",
        "\n",
        "    return np.array(windows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1FFjasvZAv-h"
      },
      "outputs": [],
      "source": [
        "# Post-processing for predictions\n",
        "def post_process_predictions(predictions, threshold=0.5, smoothing_window=5):\n",
        "    \"\"\"\n",
        "    Apply post-processing to raw predictions:\n",
        "    1. Threshold probabilities\n",
        "    2. Apply smoothing to reduce noise\n",
        "    3. Return predicted class indices and labels\n",
        "    \"\"\"\n",
        "    # Get predicted class indices\n",
        "    pred_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Apply smoothing with a sliding window\n",
        "    if len(pred_indices) >= smoothing_window:\n",
        "        smoothed_indices = np.zeros_like(pred_indices)\n",
        "        for i in range(len(pred_indices)):\n",
        "            start = max(0, i - smoothing_window // 2)\n",
        "            end = min(len(pred_indices), i + smoothing_window // 2 + 1)\n",
        "            window = pred_indices[start:end]\n",
        "            # Most common class in window\n",
        "            values, counts = np.unique(window, return_counts=True)\n",
        "            smoothed_indices[i] = values[np.argmax(counts)]\n",
        "        pred_indices = smoothed_indices\n",
        "\n",
        "    # Map indices to activity labels (add 1 because activities are 1-indexed)\n",
        "    pred_labels = [ACTIVITIES[idx + 1] for idx in pred_indices]\n",
        "\n",
        "    return pred_indices, pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MBArNseNAzY7"
      },
      "outputs": [],
      "source": [
        "# Visualization functions\n",
        "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', cmap=None, figsize=(10, 8), normalize=False):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title += ' (Normalized)'\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.cm.Blues\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45, fontsize=12)\n",
        "    plt.yticks(tick_marks, class_names, fontsize=12)\n",
        "\n",
        "    # Add text annotations to confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label', fontsize=14)\n",
        "    plt.xlabel('Predicted label', fontsize=14)\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rnqQtFg5A2tF"
      },
      "outputs": [],
      "source": [
        "def plot_classification_metrics(y_true, y_pred, class_names, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Plot precision, recall, and F1 scores from classification results\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    # Prepare data\n",
        "    metrics = [precision, recall, f1]\n",
        "    titles = ['Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "    # Create bar charts\n",
        "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        axes[i].bar(np.arange(len(class_names)), metric, color='skyblue')\n",
        "        axes[i].set_title(title, fontsize=16)\n",
        "        axes[i].set_xticks(np.arange(len(class_names)))\n",
        "        axes[i].set_xticklabels(class_names, rotation=45, ha='right', fontsize=12)\n",
        "        axes[i].set_ylim(0, 1.0)\n",
        "        axes[i].set_xlabel('Activity', fontsize=14)\n",
        "        axes[i].set_ylabel('Score', fontsize=14)\n",
        "\n",
        "        # Add value labels\n",
        "        for j, v in enumerate(metric):\n",
        "            axes[i].text(j, v + 0.02, f'{v:.2f}', ha='center', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "WjkhDSicA7yu"
      },
      "outputs": [],
      "source": [
        "def plot_combined_metrics(y_true, y_pred, class_names, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Plot a combined view of precision, recall, and F1 scores\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Set up positions for grouped bars\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.25\n",
        "\n",
        "    # Create bar groups\n",
        "    plt.bar(x - width, precision, width, label='Precision', color='#5DA5DA')\n",
        "    plt.bar(x, recall, width, label='Recall', color='#FAA43A')\n",
        "    plt.bar(x + width, f1, width, label='F1 Score', color='#60BD68')\n",
        "\n",
        "    # Add labels and titles\n",
        "    plt.title('Classification Performance Metrics by Activity', fontsize=16)\n",
        "    plt.xlabel('Activity', fontsize=14)\n",
        "    plt.ylabel('Score', fontsize=14)\n",
        "    plt.xticks(x, class_names, rotation=45, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0XhCvF61A_7v"
      },
      "outputs": [],
      "source": [
        "# Configure session settings\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "\n",
        "# Start a session\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "\n",
        "# Initializing parameters\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "n_hidden = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ycR_7vtfBGXg"
      },
      "outputs": [],
      "source": [
        "# Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVc07WulBKGM",
        "outputId": "f7f15554-e268-4302-96af-cbea576333d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
            "<ipython-input-37-2fc0c20de1db>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(filename, delim_whitespace=True, header=None)\n"
          ]
        }
      ],
      "source": [
        "# Loading the train and test data\n",
        "X_train, X_test, Y_train, Y_test, Y_train_raw, Y_test_raw = load_data()\n",
        "\n",
        "timesteps = len(X_train[0])\n",
        "input_dim = len(X_train[0][0])\n",
        "n_classes = _count_classes(Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUBr2cf_Cone",
        "outputId": "0d8087d8-2304-4258-d418-c2b879e379da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps: 128\n",
            "Input dimensions: 9\n",
            "Number of training samples: 7352\n",
            "Shape of training data: (7352, 128, 9)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Timesteps: {timesteps}\")\n",
        "print(f\"Input dimensions: {input_dim}\")\n",
        "print(f\"Number of training samples: {len(X_train)}\")\n",
        "print(f\"Shape of training data: {X_train.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XTdgzttCCqlt"
      },
      "outputs": [],
      "source": [
        "# Calculate and store normalization parameters for external data processing\n",
        "train_mean = np.mean(X_train.reshape(-1, input_dim), axis=0)\n",
        "train_std = np.std(X_train.reshape(-1, input_dim), axis=0)\n",
        "np.save('train_normalization_params.npy', {'mean': train_mean, 'std': train_std})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qD3ZyxwsBLAH"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Defining LSTM Network model:\"\"\"\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_lstm_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Add ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "MKoeA8PQBWG3",
        "outputId": "01188b7d-78cf-4146-d2d4-f42115a74206"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m37,888\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m41,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m198\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">37,888</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,278\u001b[0m (321.40 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,278</span> (321.40 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m81,830\u001b[0m (319.65 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,830</span> (319.65 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initializing the sequential model with improved architecture\n",
        "model1 = Sequential()\n",
        "\n",
        "# First LSTM layer with increased units (64) and return_sequences=True\n",
        "model1.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.5))  # Dropout after first LSTM layer\n",
        "\n",
        "# Second LSTM layer\n",
        "model1.add(Bidirectional(LSTM(n_hidden)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer with ReLU activation and L2 regularization\n",
        "model1.add(Dense(n_hidden, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "# Output layer with softmax activation\n",
        "model1.add(Dense(n_classes, activation='softmax'))\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxIvc_jBZW5",
        "outputId": "dd312318-53d6-4a70-ade7-87786a5a2bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4415 - loss: 2.0052\n",
            "Epoch 1: val_accuracy improved from -inf to 0.73295, saving model to best_lstm_model.keras\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4417 - loss: 2.0042 - val_accuracy: 0.7329 - val_loss: 0.9707 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m458/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6750 - loss: 1.0863\n",
            "Epoch 2: val_accuracy improved from 0.73295 to 0.82423, saving model to best_lstm_model.keras\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.6752 - loss: 1.0856 - val_accuracy: 0.8242 - val_loss: 0.6632 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7918 - loss: 0.7461\n",
            "Epoch 3: val_accuracy improved from 0.82423 to 0.87682, saving model to best_lstm_model.keras\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7919 - loss: 0.7459 - val_accuracy: 0.8768 - val_loss: 0.4381 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8738 - loss: 0.4903\n",
            "Epoch 4: val_accuracy improved from 0.87682 to 0.91177, saving model to best_lstm_model.keras\n",
            "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.8738 - loss: 0.4902 - val_accuracy: 0.9118 - val_loss: 0.3075 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m459/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8913 - loss: 0.4016"
          ]
        }
      ],
      "source": [
        "# Compiling the model with Adam optimizer and learning rate of 0.001\n",
        "model1.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model with early stopping, checkpoints, and reduce LR\n",
        "lstm_history = model1.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_test, Y_test),\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USUQxUoKBbGH"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "score = model1.evaluate(X_test, Y_test)\n",
        "print(\"Accuracy: \", score[1])\n",
        "print(\"Loss: \", score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-VmReygBwOj"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for confusion matrix\n",
        "y_pred = model1.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Get activity names for visualization\n",
        "activity_names = [ACTIVITIES[i+1] for i in range(n_classes)]\n",
        "\n",
        "# Generate and display enhanced confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "# Regular confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='LSTM Confusion Matrix', figsize=(12, 10))\n",
        "plt.savefig('lstm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='LSTM Confusion Matrix', figsize=(12, 10), normalize=True)\n",
        "plt.savefig('lstm_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Display text classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=activity_names))\n",
        "\n",
        "# Plot graphical classification report\n",
        "plot_classification_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('lstm_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot combined metrics\n",
        "plot_combined_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('lstm_combined_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXm7QGKtBzjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"## Defining CNN-LSTM Network model:\"\"\"\n",
        "\n",
        "\"\"\"## Defining CNN-LSTM Network model:\"\"\"\n",
        "\n",
        "# Reshape data for CNN-LSTM model\n",
        "n_steps, n_length = 4, 32\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], n_steps, n_length, input_dim))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], n_steps, n_length, input_dim))\n",
        "\n",
        "print(f\"Reshaped data for CNN-LSTM: {X_train_cnn.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g20xOPXRB3av"
      },
      "outputs": [],
      "source": [
        "# Define model with improved architecture\n",
        "model2 = Sequential()\n",
        "\n",
        "# First Conv1D layer with increased filters (128) and larger kernel size (5)\n",
        "model2.add(TimeDistributed(Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "                         input_shape=(None, n_length, input_dim)))\n",
        "model2.add(TimeDistributed(BatchNormalization()))\n",
        "model2.add(TimeDistributed(Dropout(0.5)))\n",
        "\n",
        "# Second Conv1D layer with 64 filters\n",
        "model2.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "model2.add(TimeDistributed(BatchNormalization()))\n",
        "model2.add(TimeDistributed(Dropout(0.5)))\n",
        "\n",
        "model2.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model2.add(TimeDistributed(Dropout(0.5)))  # Dropout after MaxPooling\n",
        "\n",
        "model2.add(TimeDistributed(Flatten()))\n",
        "model2.add(BatchNormalization())\n",
        "\n",
        "# First LSTM layer with return_sequences=True\n",
        "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "# Second LSTM layer\n",
        "model2.add(Bidirectional(LSTM(n_hidden)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer with ReLU activation and L2 regularization\n",
        "model2.add(Dense(n_hidden, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "# Output layer with softmax activation\n",
        "model2.add(Dense(n_classes, activation='softmax'))\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMiT2798B66l"
      },
      "outputs": [],
      "source": [
        "# Compile model with Adam optimizer and learning rate of 0.001\n",
        "model2.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Define additional callbacks for CNN-LSTM\n",
        "cnn_lstm_checkpoint = ModelCheckpoint(\n",
        "    'best_cnn_lstm_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training the model with all callbacks\n",
        "cnn_lstm_history = model2.fit(\n",
        "    X_train_cnn,\n",
        "    Y_train,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_test_cnn, Y_test),\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping, cnn_lstm_checkpoint, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3-ppIzMB_Zf"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "score = model2.evaluate(X_test_cnn, Y_test)\n",
        "print(\"Accuracy: \", score[1])\n",
        "print(\"Loss: \", score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7w3xQvLCAaS"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for confusion matrix\n",
        "y_pred = model2.predict(X_test_cnn)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate and display enhanced confusion matrix for CNN-LSTM\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Regular confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='CNN-LSTM Confusion Matrix', figsize=(12, 10))\n",
        "plt.savefig('cnn_lstm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix\n",
        "plot_confusion_matrix(cm, activity_names, title='CNN-LSTM Confusion Matrix', figsize=(12, 10), normalize=True)\n",
        "plt.savefig('cnn_lstm_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Display text classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=activity_names))\n",
        "\n",
        "# Plot graphical classification report\n",
        "plot_classification_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('cnn_lstm_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot combined metrics\n",
        "plot_combined_metrics(y_true_classes, y_pred_classes, activity_names)\n",
        "plt.savefig('cnn_lstm_combined_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkUHr6t-CKqf"
      },
      "outputs": [],
      "source": [
        "def plot_model_history(history, model_name):\n",
        "    \"\"\"\n",
        "    Plots the training and validation accuracy and loss curves for a given model.\n",
        "\n",
        "    Args:\n",
        "        history: The training history object returned by model.fit.\n",
        "        model_name: The name of the model (e.g., \"LSTM\", \"CNN-LSTM\").\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    axs[0].plot(history.history['accuracy'], linewidth=2)\n",
        "    axs[0].plot(history.history['val_accuracy'], linewidth=2)\n",
        "    axs[0].set_title(f'{model_name} Model Accuracy', fontsize=16)\n",
        "    axs[0].set_ylabel('Accuracy', fontsize=14)\n",
        "    axs[0].set_xlabel('Epoch', fontsize=14)\n",
        "    axs[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "    axs[0].legend(['Train', 'Validation'], loc='lower right', fontsize=12)\n",
        "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    axs[1].plot(history.history['loss'], linewidth=2)\n",
        "    axs[1].plot(history.history['val_loss'], linewidth=2)\n",
        "    axs[1].set_title(f'{model_name} Model Loss', fontsize=16)\n",
        "    axs[1].set_ylabel('Loss', fontsize=14)\n",
        "    axs[1].set_xlabel('Epoch', fontsize=14)\n",
        "    axs[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "    axs[1].legend(['Train', 'Validation'], loc='upper right', fontsize=12)\n",
        "    axs[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(f'{model_name.lower().replace(\" \", \"_\")}_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovyiZTliCQ_x"
      },
      "outputs": [],
      "source": [
        "# Plot the training history graphs\n",
        "plot_model_history(lstm_history, \"Improved Bidirectional LSTM\")\n",
        "plot_model_history(cnn_lstm_history, \"Improved Bidirectional CNN-LSTM\")\n",
        "\n",
        "# Save models in multiple formats\n",
        "model1.save('improved_lstm_model.keras')\n",
        "model1.save('improved_lstm.keras')\n",
        "\n",
        "model2.save('improved_cnn_lstm_model.keras')\n",
        "model2.save('improved_cnn_lstm.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE9G0b3XOyEh"
      },
      "outputs": [],
      "source": [
        "# Plot model comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(lstm_history.history['accuracy'], label='LSTM Train', linewidth=2, marker='o', markersize=4)\n",
        "plt.plot(lstm_history.history['val_accuracy'], label='LSTM Validation', linewidth=2, marker='s', markersize=4)\n",
        "plt.plot(cnn_lstm_history.history['accuracy'], label='CNN-LSTM Train', linewidth=2, marker='^', markersize=4)\n",
        "plt.plot(cnn_lstm_history.history['val_accuracy'], label='CNN-LSTM Validation', linewidth=2, marker='d', markersize=4)\n",
        "plt.title('Model Accuracy Comparison', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(lstm_history.history['loss'], label='LSTM Train', linewidth=2, marker='o', markersize=4)\n",
        "plt.plot(lstm_history.history['val_loss'], label='LSTM Validation', linewidth=2, marker='s', markersize=4)\n",
        "plt.plot(cnn_lstm_history.history['loss'], label='CNN-LSTM Train', linewidth=2, marker='^', markersize=4)\n",
        "plt.plot(cnn_lstm_history.history['val_loss'], label='CNN-LSTM Validation', linewidth=2, marker='d', markersize=4)\n",
        "plt.title('Model Loss Comparison', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Model performance comparison\n",
        "lstm_acc = score[1]\n",
        "cnn_lstm_acc = model2.evaluate(X_test_cnn, Y_test)[1]\n",
        "\n",
        "# Compare model performance with a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "models = ['Bidirectional LSTM', 'Bidirectional CNN-LSTM']\n",
        "accuracies = [lstm_acc, cnn_lstm_acc]\n",
        "\n",
        "# Create a gradient-colored bar chart\n",
        "plt.bar(models, accuracies, color=['#3498db', '#2ecc71'], alpha=0.8, width=0.6)\n",
        "plt.title('Model Accuracy Comparison', fontsize=16)\n",
        "plt.ylabel('Test Accuracy', fontsize=14)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}